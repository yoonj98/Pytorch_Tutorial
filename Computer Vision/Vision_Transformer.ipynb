{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Vision_Transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvhxX0KHhaSjqxUYwCq0h0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# !pip install timm pandas requests"],"metadata":{"id":"Ba_jx3k3u00I"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIu8wmMGuw7W"},"outputs":[],"source":["from PIL import Image\n","import torch\n","import timm\n","import requests\n","import torchvision.transforms as transforms\n","from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","\n","print(torch.__version__)\n","# Pytorch 버전은 1.8.0 이어야 합니다.\n","\n","\n","model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n","model.eval()\n","\n","transform = transforms.Compose([\n","    transforms.Resize(256, interpolation=3),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n","])\n","\n","img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n","img = transform(img)[None,]\n","out = model(img)\n","clsidx = torch.argmax(out)\n","print(clsidx.item())"]},{"cell_type":"code","source":["model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n","model.eval()\n","scripted_model = torch.jit.script(model)\n","scripted_model.save(\"fbdeit_scripted.pt\")"],"metadata":{"id":"XBmUtNHVu-kI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 서버 추론을 위해 'fbgemm'을, 모바일 추론을 위해 'qnnpack'을 사용해 봅시다.\n","backend = \"fbgemm\" # 이 주피터 노트북에서는 양자화된 모델의 더 느린 추론 속도를 일으키는 qnnpack으로 대체되었습니다.\n","model.qconfig = torch.quantization.get_default_qconfig(backend)\n","torch.backends.quantized.engine = backend\n","\n","quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n","scripted_quantized_model = torch.jit.script(quantized_model)\n","scripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")"],"metadata":{"id":"zAMnbpdUvAWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = scripted_quantized_model(img)\n","clsidx = torch.argmax(out)\n","print(clsidx.item())"],"metadata":{"id":"nyt5fQBJvBi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.mobile_optimizer import optimize_for_mobile\n","optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n","optimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")"],"metadata":{"id":"Yn7nSPIIvCdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out = optimized_scripted_quantized_model(img)\n","clsidx = torch.argmax(out)\n","print(clsidx.item())"],"metadata":{"id":"ZdWQCw87vDMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\n","ptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")"],"metadata":{"id":"cARLEKz6vEw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n","    out = model(img)\n","with torch.autograd.profiler.profile(use_cuda=False) as prof2:\n","    out = scripted_model(img)\n","with torch.autograd.profiler.profile(use_cuda=False) as prof3:\n","    out = scripted_quantized_model(img)\n","with torch.autograd.profiler.profile(use_cuda=False) as prof4:\n","    out = optimized_scripted_quantized_model(img)\n","with torch.autograd.profiler.profile(use_cuda=False) as prof5:\n","    out = ptl(img)\n","\n","print(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\n","print(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\n","print(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\n","print(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\n","print(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))"],"metadata":{"id":"ZmjIvI1OvFpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\n","df = pd.concat([df, pd.DataFrame([\n","    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n","    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n","     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n","    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n","     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n","    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n","     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n","    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n","     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n","    columns=['Inference Time', 'Reduction'])], axis=1)\n","\n","print(df)\n","\n","\"\"\"\n","        Model                             Inference Time    Reduction\n","0   original model                             1236.69ms           0%\n","1   scripted model                             1226.72ms        0.81%\n","2   scripted & quantized model                  593.19ms       52.03%\n","3   scripted & quantized & optimized model      598.01ms       51.64%\n","4   lite model                                  600.72ms       51.43%\n","\"\"\""],"metadata":{"id":"HKY5ownMvIXh"},"execution_count":null,"outputs":[]}]}
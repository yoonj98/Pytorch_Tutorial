{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transfer_Learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMN0FN/R7H5E0GwRgUBYYGa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 전이학습 (Transfer Learning) 이란\n","1. **합성곱 신경망의 미세조정(finetuning)** : 무작위 초기화 대신, 신경망을 ImageNet 1000 데이터셋 등으로 미리 학습한 신경망으로 초기화합니다. 학습의 나머지 과정들은 평상시와 동일함.  \n","2. **고정된 특징 추출기로써의 합성곱 신경망** : 마지막에 완전히 연결 된 계층을 제외한 모든 신경망의 가중치를 고정합니다. 이 마지막의 완전히 연결된 계층은 새로운 무작위의 가중치를 갖는 계층으로 대체되어 이 계층만 학습합니다."],"metadata":{"id":"ERGI7ROTpDNp"}},{"cell_type":"markdown","source":["---\n","#### 1. Load Data\n","* Task  \n","개미와 벌 분류  \n"," \n","* Dataset  \n","Train / Validation : 120 / 75"],"metadata":{"id":"Fvi6rC4LpXk9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjt2omiBo-Jy"},"outputs":[],"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","plt.ion()   # 대화형 모드\n","\n","# 학습을 위해 데이터 증가(augmentation) 및 일반화(normalization)\n","# 검증을 위한 일반화\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","data_dir = 'data/hymenoptera_data'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n","                                             shuffle=True, num_workers=4)\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","source":["#### 1-1. Visulaize Image"],"metadata":{"id":"Oygh7ZYnppgn"}},{"cell_type":"code","source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # 갱신이 될 때까지 잠시 기다립니다.\n","\n","\n","# 학습 데이터의 배치를 얻습니다.\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","# 배치로부터 격자 형태의 이미지를 만듭니다.\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out, title=[class_names[x] for x in classes])"],"metadata":{"id":"WmLaMDjGppHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Model Train"],"metadata":{"id":"lRJxxjMQp21v"}},{"cell_type":"code","source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # 각 에폭(epoch)은 학습 단계와 검증 단계를 갖습니다.\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # 모델을 학습 모드로 설정\n","            else:\n","                model.eval()   # 모델을 평가 모드로 설정\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # 데이터를 반복\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # 매개변수 경사도를 0으로 설정\n","                optimizer.zero_grad()\n","\n","                # 순전파\n","                # 학습 시에만 연산 기록을 추적\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # 학습 단계인 경우 역전파 + 최적화\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # 통계\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            # 모델을 깊은 복사(deep copy)함\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # 가장 나은 모델 가중치를 불러옴\n","    model.load_state_dict(best_model_wts)\n","    return model"],"metadata":{"id":"bFFtQ2W6pzlz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3-1. 합성곱 신경망의 미세조정(finetuning) \n","미리 학습한 모델 불러와 fully-connected layer 초기화"],"metadata":{"id":"fdAOJ2QOp9Iv"}},{"cell_type":"code","source":["model_ft = models.resnet18(pretrained=True)\n","num_ftrs = model_ft.fc.in_features\n","# 여기서 각 출력 샘플의 크기는 2로 설정합니다.\n","# 또는, nn.Linear(num_ftrs, len (class_names))로 일반화할 수 있습니다.\n","model_ft.fc = nn.Linear(num_ftrs, 2)\n","\n","model_ft = model_ft.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# 모든 매개변수들이 최적화되었는지 관찰\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# 7 에폭마다 0.1씩 학습률 감소\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"metadata":{"id":"U5M8kClLqGT6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3-1-1. Model Inference"],"metadata":{"id":"lgyRkyL3qMgt"}},{"cell_type":"code","source":["model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=25)"],"metadata":{"id":"E81rAkQ6qMQq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3-1-2. Visualize Predicted Data"],"metadata":{"id":"5u_3sfPlp0el"}},{"cell_type":"code","source":["def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders['val']):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)\n","\n","visualize_model(model_ft)"],"metadata":{"id":"Abce5zKkp8jy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3-2. 고정된 특징 추출기로써의 합성곱 신경망\n","마지막 계층을 제외한 신경망의 모든 부분을 고정해야 하므로, requires_grad == False 로 설정하여 매개변수를 고정하여 backward() 중에 경사도가 계산되지 않도록 함."],"metadata":{"id":"YCJdV2wrqimc"}},{"cell_type":"code","source":["model_conv = torchvision.models.resnet18(pretrained=True)\n","\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","# 새로 생성된 모듈의 매개변수는 기본값이 requires_grad=True 임\n","num_ftrs = model_conv.fc.in_features\n","model_conv.fc = nn.Linear(num_ftrs, 2)\n","model_conv = model_conv.to(device)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 이전과는 다르게 마지막 계층의 매개변수들만 최적화되는지 관찰\n","optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n","\n","# 7 에폭마다 0.1씩 학습률 감소\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"],"metadata":{"id":"dZ9aBJbHqh9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3-2-1. Model Inference"],"metadata":{"id":"re0cuYTaquSL"}},{"cell_type":"code","source":["model_conv = train_model(model_conv, criterion, optimizer_conv,\n","                         exp_lr_scheduler, num_epochs=25)\n","\n","visualize_model(model_conv)\n","\n","plt.ioff()\n","plt.show()"],"metadata":{"id":"uLqZsLkcqyUp"},"execution_count":null,"outputs":[]}]}
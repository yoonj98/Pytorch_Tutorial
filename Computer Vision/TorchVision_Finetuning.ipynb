{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TorchVision_Finetuning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Zmokyg2MKJUOqNTYDnHaeVAiNJGqa3qz","authorship_tag":"ABX9TyM7jKhdD7KcakSiSi1boRtJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53QUoLWrlljG","executionInfo":{"status":"ok","timestamp":1639882089986,"user_tz":-540,"elapsed":9864,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"067f4aea-caf8-4065-ef39-3a7d0dbb0e8f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["### Mask R-CNN Finetuning \n","* task  \n","Instance Segmentation\n","  \n","* data   \n","  보행자 인스턴스 (345명이 있는 170개의 이미지)\n","  > 이미지 내에서 사람의 위치 좌표와 픽셀 단위의 사람 여부를 구분한 정보를 포함\n","\n","---\n","#### 1. 데이터셋 정의\n","표준 torch.utils.data.Dataset 클래스를 상속 받아야 하며, __len__ 와 __getitem__ 메소드를 구현해 주어야한다.  \n","\n","데이터셋에서 필요한 유일한 특성은 __getitem__ 메소드가 다음을 반환 해야 하는 것:\n","\n","이미지 : PIL(Python Image Library) 이미지의 크기 (H, W)  \n","대상 : 다음의 필드를 포함하는 사전 타입  \n","* boxes (FloatTensor[N, 4]): N 개의 바운딩 박스(Bounding box)의 좌표를 [x0, y0, x1, y1] 형태로 가집니다. x와 관련된 값 범위는 0 부터 W 이고 y와 관련된 값의 범위는 0 부터 H 까지입니다.  \n","* labels (Int64Tensor[N]): 바운딩 박스 마다의 라벨 정보입니다. 0 은 항상 배경의 클래스를 표현합니다.  \n","* image_id (Int64Tensor[1]): 이미지 구분자입니다. 데이터셋의 모든 이미지 간에 고유한 값이어야 하며 평가 중에도 사용됩니다.  \n","* area (Tensor[N]): 바운딩 박스의 면적입니다. 면적은 평가 시 작음,중간,큰 박스 간의 점수를 내기 위한 기준이며 COCO 평가를 기준으로 합니다.  \n","* iscrowd (UInt8Tensor[N]): 이 값이 참일 경우 평가에서 제외합니다.  \n","* (선택적) masks (UInt8Tensor[N, H, W]): N 개의 객체 마다의 분할 마스크 정보입니다.  \n","* (선택적) keypoints (FloatTensor[N, K, 3]): N 개의 객체마다의 키포인트 정보입니다. 키포인트는 [x, y, visibility] 형태의 값입니다. visibility 값이 0인 경우 키포인트는 보이지 않음을 의미합니다. 데이터 증강(Data augmentation)의 경우 키포인트 좌우 반전의 개념은 데이터 표현에 따라 달라지며, 새로운 키포인트 표현에 대해 “references/detection/transforms.py” 코드 부분을 수정 해야 할 수도 있습니다.\n","\n","> 해당 모델은 labels : class 0을 배경으로 취급하므로, 어떤 이미지에 2개의 클래스가 모두 있다면 labels 텐서는 [1, 2]로 표현"],"metadata":{"id":"LSRIcizBlr3P"}},{"cell_type":"markdown","source":["각 이미지에는 해당하는 **분할 마스크**가 있으며, 여기서 각각의 색상은 다른 인스턴스에 해당한다.\n","\n","![image](https://user-images.githubusercontent.com/69336270/146662001-0096ff7b-f277-4d4a-baea-9c376fad7a6b.png)\n","![image](https://user-images.githubusercontent.com/69336270/146662012-fcaabd27-1add-47ac-bd11-7195e86496bc.png)"],"metadata":{"id":"WPjqKWVJnRIL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PR-nKyXRlSEn"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        # 모든 이미지 파일들을 읽고, 정렬하여\n","        # 이미지와 분할 마스크 정렬을 확인합니다\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n","        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n","\n","    def __getitem__(self, idx):\n","        # 이미지와 마스크를 읽어옵니다\n","        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n","        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # 분할 마스크는 RGB로 변환하지 않음을 유의하세요\n","        # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다\n","        mask = Image.open(mask_path)\n","        # numpy 배열을 PIL 이미지로 변환합니다\n","        mask = np.array(mask)\n","        # 인스턴스들은 다른 색들로 인코딩 되어 있습니다.\n","        obj_ids = np.unique(mask)\n","        # 첫번째 id 는 배경이라 제거합니다\n","        obj_ids = obj_ids[1:]\n","\n","        # 컬러 인코딩된 마스크를 바이너리 마스크 세트로 나눕니다\n","        masks = mask == obj_ids[:, None, None]\n","\n","        # 각 마스크의 바운딩 박스 좌표를 얻습니다\n","        num_objs = len(obj_ids)\n","        boxes = []\n","        for i in range(num_objs):\n","            pos = np.where(masks[i])\n","            xmin = np.min(pos[1])\n","            xmax = np.max(pos[1])\n","            ymin = np.min(pos[0])\n","            ymax = np.max(pos[0])\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","        # 모든 것을 torch.Tensor 타입으로 변환합니다\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        # 객체 종류는 한 종류만 존재합니다(역자주: 예제에서는 사람만이 대상입니다)\n","        labels = torch.ones((num_objs,), dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다\n","        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"markdown","source":["#### 2. 모델 정의\n","해당 튜토리얼에서는 FAster R-CNN에 기반한 Mask R-CNN 모델 사용\n","> * Faster R-CNN : 이미지에 존재할 수 있는 객체에 대한 바운딩 박스와 클래스 점수를 모두 예측하는 모델  \n","* Mask R-CNN은 각 인스턴스에 대한 분할 마스크 예측하는 추가 분기(레이어)를 Faster R-CNN에 추가한 모델"],"metadata":{"id":"M--87jiUncah"}},{"cell_type":"markdown","source":["#### 2-1. Pre-Trained Model로부터 Finetuning"],"metadata":{"id":"A4F3s8YSoLAp"}},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# COCO로 미리 학습된 모델 읽기\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","# 분류기를 새로운 것으로 교체하는데, num_classes는 사용자가 정의합니다\n","num_classes = 2  # 1 클래스(사람) + 배경\n","# 분류기에서 사용할 입력 특징의 차원 정보를 얻습니다\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# 미리 학습된 모델의 머리 부분을 새로운 것으로 교체합니다\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"],"metadata":{"id":"k5h0ZWZEny_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2-2. 다른 백본 추가 혹은 교체"],"metadata":{"id":"WmoPxGpGn2qU"}},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n","backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n","# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n","# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n","backbone.out_channels = 1280\n","\n","# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n","# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n","# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n","\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n","# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n","# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n","# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# 조각들을 Faster RCNN 모델로 합칩니다.\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"],"metadata":{"id":"CNwODriOn-Hk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터의 수가 매우 적기 때문에 2-1. finetuning 방법 채택"],"metadata":{"id":"xakbMN1eoCdr"}},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","    # 분류를 위한 입력 특징 차원을 얻습니다\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # 미리 학습된 헤더를 새로운 것으로 바꿉니다\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # 마스크 분류기를 위한 입력 특징들의 차원을 얻습니다\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # 마스크 예측기를 새로운 것으로 바꿉니다\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"metadata":{"id":"PwPiEeqGoIWH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Data Augmentation & Transforms"],"metadata":{"id":"Jc0tKzM7oSnm"}},{"cell_type":"code","source":["import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.ToTensor())\n","    if train:\n","        # 학습시 50% 확률로 학습 영상을 좌우 반전 변환\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"metadata":{"id":"KNz_45CmoXVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Tip. forward() method test\n","데이터셋을 반복하여 학습하기 전에, 다음과 같은 코드를 통해 샘플 데이터로 모델이 예상대로 동작하는 지 test 할수 있다."],"metadata":{"id":"c2hUm5uwodmd"}},{"cell_type":"code","source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","data_loader = torch.utils.data.DataLoader(\n"," dataset, batch_size=2, shuffle=True, num_workers=4,\n"," collate_fn=utils.collate_fn)\n","\n","# 학습 시\n","images,targets = next(iter(data_loader))\n","images = list(image for image in images)\n","targets = [{k: v for k, v in t.items()} for t in targets]\n","output = model(images,targets)   # Returns losses and detections\n","\n","# 추론 시\n","model.eval()\n","x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","predictions = model(x)           # Returns predictions"],"metadata":{"id":"uRVPwayzosn2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Model Train and Inference"],"metadata":{"id":"_4k7syW6oxge"}},{"cell_type":"code","source":["from engine import train_one_epoch, evaluate\n","import utils\n","\n","def main():\n","    # 학습을 GPU로 진행하되 GPU가 가용하지 않으면 CPU로 합니다\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    # 우리 데이터셋은 두 개의 클래스만 가집니다 - 배경과 사람\n","    num_classes = 2\n","    # 데이터셋과 정의된 변환들을 사용합니다\n","    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n","    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n","\n","    # 데이터셋을 학습용과 테스트용으로 나눕니다(역자주: 여기서는 전체의 50개를 테스트에, 나머지를 학습에 사용합니다)\n","    indices = torch.randperm(len(dataset)).tolist()\n","    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","    # 데이터 로더를 학습용과 검증용으로 정의합니다\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=2, shuffle=True, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","\n","    data_loader_test = torch.utils.data.DataLoader(\n","        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n","        collate_fn=utils.collate_fn)\n","\n","    # 도움 함수를 이용해 모델을 가져옵니다\n","    model = get_model_instance_segmentation(num_classes)\n","\n","    # 모델을 GPU나 CPU로 옮깁니다\n","    model.to(device)\n","\n","    # 옵티마이저(Optimizer)를 만듭니다\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005,\n","                                momentum=0.9, weight_decay=0.0005)\n","    # 학습률 스케쥴러를 만듭니다\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                   step_size=3,\n","                                                   gamma=0.1)\n","\n","    # 10 에포크만큼 학습해봅시다\n","    num_epochs = 10\n","\n","    for epoch in range(num_epochs):\n","        # 1 에포크동안 학습하고, 10회 마다 출력합니다\n","        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","        # 학습률을 업데이트 합니다\n","        lr_scheduler.step()\n","        # 테스트 데이터셋에서 평가를 합니다\n","        evaluate(model, data_loader_test, device=device)\n","\n","    print(\"That's it!\")"],"metadata":{"id":"5JhDl0_yowDj"},"execution_count":null,"outputs":[]}]}
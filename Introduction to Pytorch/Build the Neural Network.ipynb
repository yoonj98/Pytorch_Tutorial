{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Build the Neural Network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNva2I/4Bm8d0sGak1uE4i1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"35foaM37SjP9"},"source":["## Pytorch Tutorials : Build the Neural Network\n","- [Pytorch Tutorials](https://pytorch.org/tutorials/)\n","- [Build the Neural Network](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)"]},{"cell_type":"markdown","metadata":{"id":"XY28H4RnSjKO"},"source":["### Build the Neural Network\n","신경망은 데이터에 대한 연산을 수행하는 계층/모듈로 구성  \n","\n","`torch.nn` 네임스페이스는 자신만의 신경망을 구축하는 데 필요한 모든 구성 요소를 제공합니다. "]},{"cell_type":"code","metadata":{"id":"yQR8sT2xSiRS","executionInfo":{"status":"ok","timestamp":1631438789307,"user_tz":-540,"elapsed":2411,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}}},"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rx7I3pGtTLRF"},"source":["### Get Device for Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Av8MLgQSSiMQ","executionInfo":{"status":"ok","timestamp":1631438789309,"user_tz":-540,"elapsed":12,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"779661c7-32ba-442b-9a8d-c5a603c4b1ad"},"source":["# gpu 가용성 확인\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n"]}]},{"cell_type":"markdown","metadata":{"id":"VrHN9HvyTVUs"},"source":["### Define the Class\n","`nn.Module`을 하위 분류하여 신경망 모델을 정의한다. 또한, `__init__`의 신경망 레이어를 초기화 한다."]},{"cell_type":"code","metadata":{"id":"tuUFLpgZSiKN","executionInfo":{"status":"ok","timestamp":1631438789310,"user_tz":-540,"elapsed":10,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}}},"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0CRPLqfhTr6G"},"source":["신경망 인스턴스를 만들어서 장치로 옮기고 그 구조를 확인"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DB5g0dtDSiH3","executionInfo":{"status":"ok","timestamp":1631438789310,"user_tz":-540,"elapsed":10,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"ea57e8d9-ab3b-4e9d-c92b-a1662d04fc38"},"source":["model = NeuralNetwork().to(device)\n","print(model)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"_8qApxqUTzKa"},"source":["모델을 사용하기 위해 입력 데이터 전달 > 모델의 순전파 연산과 일부 역전파 연산이 실행됨 (굳이 model.forward() 호출하지 않아도 됨)  \n","\n","모델을 호출하면 각 클래스에 대해 예측 값을 갖는 10차원 텐서가 반환된다. 우리는 nn.Softmax module을 통과시킴으로써 예측 확률을 얻는다"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"md0q_nNbSiF2","executionInfo":{"status":"ok","timestamp":1631438789590,"user_tz":-540,"elapsed":287,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"036c7cf2-341a-4e12-b6e4-e1bc530ae867"},"source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: tensor([8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"2dlKopkXUSEA"},"source":["### Model Layers\n","FashionMNIST 모델의 레이어를 분해해 봅시다. 이를 설명하기 위해 28x28 사이즈의 이미지 3개로 구성된 미니배치 샘플을 채취하여 네트워크를 통과할 때 어떤 일이 일어나는지 알아보겠습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jwAd_ysSiDt","executionInfo":{"status":"ok","timestamp":1631438789590,"user_tz":-540,"elapsed":7,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"1b161ab3-19a7-4bc1-ce16-5816639f2404"},"source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}]},{"cell_type":"markdown","metadata":{"id":"H2NnuWhSUYec"},"source":["nn.Flatten()  \n","레이어를 초기화하여 각 2D 28x28 이미지를 784픽셀 값의 연속 배열로 변환 (미니 배치 차원(dim=0)이 유지됨)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTw_GmheSiBu","executionInfo":{"status":"ok","timestamp":1631438789975,"user_tz":-540,"elapsed":5,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"c220d5c1-e0e7-477b-8e4c-68d481b6f357"},"source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}]},{"cell_type":"markdown","metadata":{"id":"PtUaSP2gUg0w"},"source":["nn.Linear()  \n","그 저장 가중치와 바이어스를 이용하여 상기 입력에 선형 변환을 적용시키는 모듈"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fq8pxu_rUfhv","executionInfo":{"status":"ok","timestamp":1631438791600,"user_tz":-540,"elapsed":279,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"92c6b592-a73d-4871-d93d-cb71115f70e3"},"source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}]},{"cell_type":"markdown","metadata":{"id":"TF7sLO7SUptQ"},"source":["nn.ReLU()  \n","비선형 활성화는 모델의 입력과 출력 사이에 복잡한 매핑을 생성합니다. 선형 변환 후에 적용하여 비선형성을 도입 하여 신경망이 다양한 현상을 학습하도록 돕습니다.\n","\n","이 모델에서는 선형 레이어 사이에 nn.ReLU 를 사용 하지만 모델에 비선형성을 도입하기 위한 다른 활성화가 있습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed9dOIxZUffc","executionInfo":{"status":"ok","timestamp":1631438793167,"user_tz":-540,"elapsed":385,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"ffff5b05-b510-49cc-bfbe-99d5557b29b4"},"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[-0.2244, -0.0535, -0.5564,  0.0033, -0.2413,  0.3353,  0.9000, -0.6146,\n","          0.1706, -0.3343, -0.1026,  0.6228,  0.0440,  0.3048, -0.2537,  0.2835,\n","         -0.0936,  0.2472, -0.2011, -0.1276],\n","        [-0.2678,  0.0575, -0.3276,  0.1067, -0.5123,  0.5603,  0.6656, -0.3501,\n","          0.1722, -0.3640, -0.0359,  0.6597,  0.1392, -0.0702, -0.3290,  0.5670,\n","         -0.0970,  0.2720,  0.0648, -0.1461],\n","        [ 0.0612, -0.1240, -0.3355,  0.0852, -0.4117,  0.2753,  0.8200, -0.3926,\n","          0.0762, -0.4082, -0.1469,  0.7591,  0.1468,  0.2095, -0.1065,  0.3736,\n","          0.4928,  0.2663, -0.0858,  0.0950]], grad_fn=<AddmmBackward>)\n","\n","\n","After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0033, 0.0000, 0.3353, 0.9000, 0.0000, 0.1706,\n","         0.0000, 0.0000, 0.6228, 0.0440, 0.3048, 0.0000, 0.2835, 0.0000, 0.2472,\n","         0.0000, 0.0000],\n","        [0.0000, 0.0575, 0.0000, 0.1067, 0.0000, 0.5603, 0.6656, 0.0000, 0.1722,\n","         0.0000, 0.0000, 0.6597, 0.1392, 0.0000, 0.0000, 0.5670, 0.0000, 0.2720,\n","         0.0648, 0.0000],\n","        [0.0612, 0.0000, 0.0000, 0.0852, 0.0000, 0.2753, 0.8200, 0.0000, 0.0762,\n","         0.0000, 0.0000, 0.7591, 0.1468, 0.2095, 0.0000, 0.3736, 0.4928, 0.2663,\n","         0.0000, 0.0950]], grad_fn=<ReluBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6dcC6zuHUvvs"},"source":["nn.Sequential()  \n","모듈의 순서가 지정된 컨테이너로, 데이터는 정의된 것과 동일한 순서로 모든 모듈을 통해 전달됩니다."]},{"cell_type":"code","metadata":{"id":"4jHr98RSUfc0","executionInfo":{"status":"ok","timestamp":1631438795370,"user_tz":-540,"elapsed":374,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}}},"source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_ueiEsBU4HE"},"source":["nn.Softmax()  \n","신경망의 마지막 선형 계층은 nn.Softmax 모듈에 전달되는 [-infty, infty]의 원시 값인 로짓을 반환 합니다. 로짓은 각 클래스에 대한 모델의 예측 확률을 나타내는 값 [0, 1]로 조정됩니다. 매개변수 `dim`는 값의 합이 1이 되어야 하는 차원을 나타냅니다."]},{"cell_type":"code","metadata":{"id":"-NuGfjmsUfau","executionInfo":{"status":"ok","timestamp":1631438796313,"user_tz":-540,"elapsed":2,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}}},"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmoNAoF4U-7s"},"source":["### 모델 매개변수\n","신경망 내부의 많은 레이어는 매개변수화되어 있습니다 . 즉, 훈련 중에 최적화된 관련 가중치와 편향이 있습니다. nn.Module은 모델 개체 내부에 정의된 모든 필드를 자동으로 추적하고 모델 parameters()또는 named_parameters()메서드를 사용하여 모든 매개변수에 액세스할 수 있도록 합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efN1oRFWUfYM","executionInfo":{"status":"ok","timestamp":1631438797007,"user_tz":-540,"elapsed":372,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"c014211b-902a-41bc-b1d2-a9c1105df489"},"source":["print(\"Model structure: \", model, \"\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure:  NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",") \n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0280, -0.0300,  0.0334,  ..., -0.0253,  0.0347, -0.0325],\n","        [ 0.0040, -0.0262,  0.0309,  ..., -0.0012, -0.0200, -0.0329]],\n","       grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0103, -0.0052], grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0082,  0.0017,  0.0385,  ...,  0.0134,  0.0437,  0.0043],\n","        [-0.0110,  0.0001,  0.0363,  ..., -0.0052,  0.0169, -0.0392]],\n","       grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0331, -0.0030], grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0047,  0.0378, -0.0268,  ...,  0.0142, -0.0276, -0.0022],\n","        [-0.0142, -0.0200,  0.0300,  ..., -0.0391,  0.0326, -0.0430]],\n","       grad_fn=<SliceBackward>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0154,  0.0281], grad_fn=<SliceBackward>) \n","\n"]}]}]}
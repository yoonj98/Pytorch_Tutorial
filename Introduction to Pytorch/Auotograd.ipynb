{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Auotograd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMT+F5t0l2KRuhjCX7W1HuB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vEOE9nmXVSN8"},"source":["## Pytorch Tutorials : Auotograd\n","- [Pytorch Tutorials](https://pytorch.org/tutorials/)\n","- [Auotograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)"]},{"cell_type":"markdown","metadata":{"id":"GsSW4W0mVSHt"},"source":["### Automatic Differentiation(자동 미분) with torch.autograd\n","신경망을 훈련할 때 가장 자주 사용되는 알고리즘은 역전파로,  매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수 의 기울기에 따라 조정됩니다 .\n","\n","이러한 그라디언트를 계산하기 위해 PyTorch에는 `torch.autograd`가 내장되어 있으며, 모든 계산 그래프에 대한 기울기 자동 계산을 지원합니다. 입력으로 간단한 하나의 층 신경망을 고려 x파라미터 w와 b, 일부 손실 함수. 다음과 같은 방식으로 PyTorch에서 정의할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"HMmpz2r_VQRo","executionInfo":{"status":"ok","timestamp":1631440276894,"user_tz":-540,"elapsed":2809,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}}},"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_ulcN0aYGE8"},"source":["### Tensors, Functions and Computational graph\n","![](https://pytorch.org/tutorials/_images/comp-graph.png)\n","다음과 같은 계산 그래프를 정의할 수 있음.\n","\n","이 네트워크에서 w와 b는 최적화해야 하는 매개변수이므로, 이러한 변수와 관련하여 손실 함수의 그래디언트를 계산할 수 있어야 한다. 이를 위해 텐서의 `required_grad` 속성을 설정합니다.(텐서를 생성할 때 또는 나중에 `x.required_grad_(True)` 메서드를 사용하여 `required_grad` 값을 설정할 수 있습니다.)"]},{"cell_type":"markdown","metadata":{"id":"kOIPvGZ_YpQj"},"source":["계산 그래프를 구성하기 위해 텐서에 적용하는 함수는 클래스 함수의 객체로, 함수를 순전파로 계산하는 방법과 역전파 단계에서 파생 함수를 계산하는 방법을 알고 있습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"laj6t5Q-XwuG","executionInfo":{"status":"ok","timestamp":1631440276895,"user_tz":-540,"elapsed":11,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"9d099dfa-91c2-4d63-ed94-caa5b6f77a64"},"source":["print('Gradient function for z =', z.grad_fn)\n","print('Gradient function for loss =', loss.grad_fn)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7f5b5f0189d0>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f5b5f018a50>\n"]}]},{"cell_type":"markdown","metadata":{"id":"Hzf48ZoVYyb8"},"source":["### Computing Gradients\n","신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실 함수의 도함수를 계산해야 함.  \n","\n","이러한 도함수를 계산하기 위해 `loss.backward()`을 선언하고, w.grad 및 b.grad에서 값을 계산한다.\n","\n","> 성능상의 이유로 주어진 그래프에서 `backward`를 한 번만 사용하여 그레이디언트 계산을 수행할 수 있습니다. 같은 그래프에서 여러 번의 `backward`를 해야 한다면 `retain_graph=True`을 `backward` 넘겨야한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfDeRPS4Xwqz","executionInfo":{"status":"ok","timestamp":1631440277275,"user_tz":-540,"elapsed":385,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"33a644f0-ca47-4885-bf2e-68250e19ae9a"},"source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0352, 0.0399, 0.2644],\n","        [0.0352, 0.0399, 0.2644],\n","        [0.0352, 0.0399, 0.2644],\n","        [0.0352, 0.0399, 0.2644],\n","        [0.0352, 0.0399, 0.2644]])\n","tensor([0.0352, 0.0399, 0.2644])\n"]}]},{"cell_type":"markdown","metadata":{"id":"d_lt3DJxZmwL"},"source":["### Disabling Gradient Tracking\n","기본적으로 `retain_graph=True`가 있는 모든 텐서는 계산 이력을 추적하고 그래디언트 계산을 지원한다. 그러나 예를 들어 모델을 교육하고 일부 입력 데이터에만 적용하려는 경우, 즉 forward 연산만 수행하려는 경우처럼 그렇게 할 필요가 없는 경우가 있습니다. 연산 코드를 `torch.no_grad` 블록으로 둘러싸면 연산 추적을 중지할 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZf3H8n-Xwlr","executionInfo":{"status":"ok","timestamp":1631440277276,"user_tz":-540,"elapsed":13,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"d243a9d8-d694-4e7a-88f3-8ce8eee8388b"},"source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"markdown","metadata":{"id":"jPtpr-xjZ5QN"},"source":["`detach()`를 통해 동일한 결과를 반환할 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftGYTEo_XwjT","executionInfo":{"status":"ok","timestamp":1631440277276,"user_tz":-540,"elapsed":7,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"2e932e39-d1ee-45f6-d964-31c07adbdc20"},"source":["z = torch.matmul(x, w)+b\n","z_det = z.detach()\n","print(z_det.requires_grad)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ig3PSwTfaGlr"},"source":["Disabling Gradient Tracking 사용 이유\n"," 1. 신경망의 일부 매개 변수를 동결 - 사전훈련된 신경망을 finetuning하기 위함\n"," 2. 그래디언트를 추적하지 않는 tensor에 대한 계산이 더욱 효율적이기 때문에 순전파 계산만 하는 경우 연산 속도를 높일 수 있다."]}]}